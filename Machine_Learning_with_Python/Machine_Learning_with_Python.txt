1. Introduction to Machine Learning
    1.1 What is machine learning?
        : Machine learning is the subfield of computer science that gives "computers the ability to learn without being explicitly programmed."
    1.2 Major machine learning techniques
        1.2.1 Regression/Estimation: Predicting continuous values
        1.2.2 Classification: Predicting the item class/category of a case
        1.2.3 Clustering: Finding the structure of data; summarization
        1.2.4 Associations: Associating frequent co-occuring items/events
        1.2.5 Anomaly detection: Discovering abnormal and unusual cases
        1.2.6 Sequence mining: Predicting next events; click-stream (Markov Model, HMM)
        1.2.7 Dimension Reduction: Reducing the size of data (PCA)
        1.2.8 Recomendation systems: Recommending items
    1.3 Python for Machine Learning
        1.3.1 python libraries for machine learning
            1.3.1.1 NumPy
            1.3.1.2 SciPy
            1.3.1.3 matplotlib
            1.3.1.4 pandas
            1.3.1.5 scikit-learn
	    - Free software machine learning library
	    - Classification, Regression and Clustering algorithms
	    - Works with NumPy and SciPy
	    - Great docmentation
	    - Easy to implement
    1.4 Supervised vs Unsupervised
        1.4.1 What is supervised learning?
            : Teaching the model with labeled data
        1.4.2 Types of supervised learning
            - Classification: Classification is the process of predicting discrete class labels or categories.
            - Regression: Regression is the process of predicting continuous values.
        1.4.3 What is unsupervised learning?
	: The model works on its own to discover information
        1.4.4 Types of unsupervised learning
            - Dimension reduction
            - Density estimation
            - Market basket analysis
            - Clustering: Clustering is grouping of data points or objects that are somehow similar by discvoering structure, summarization and anomaly detection

2. Regression
    2.1 Introduction to Regression
        2.1.1 What is regression?
	: Regression is the process of predicting a continuous value(dependent variable) with independent variable(s)
        2.1.2 What is a regression model?
            - Simple regression model (predicting dependent variable with one independent variable)
            - Multiple regression model (predicting dependent variable with more than one independent variable)
        2.1.3 Applications of regression
            - Sales forecasting
            - Satisfaction analysis
            - Price estimation
            - Employment income
        2.1.4 Regression algorithms
            - Oridinal regression
            - Poisson regression
            - Fast forest quantile regression
            - Linear, Polynomial, Lasso, Stepwise, Ridge regression
            - Bayesian linear regression
            - Neural network regression
            - Decision forest regression
            - Boosted decision tree regression
            - KNN (K-nearest neighbors)
    2.2 Simple Linear Regression 
        : Predicting dependent variable with one independent variable
    2.3 Model Evaluation in Regression Models
        2.2.1 Model evaluation approaches
            2.2.1.1 Train and Test on the same dataset
	  - High training accuracy isn't necessarily a good thing
	  - Result of overf-fitting
            2.2.1.2 Train / Test Split
	  - It's important that our models have a high, out-of-sample accuracy
	  - More accuracte evaluation on out-of-sample accuracy
	  - Highly depedent on which datasets the date is trained and tested
        2.2.2 K-fold cross-validation
    2.4 Evaluation Metrics in regression models
        2.4.1 What is an error of the model?
	- MAE (Mean Absolute Error)
	- MSE (Mean Squared Error)
	- RMSE (Root Mean Squared Error)
	- RAE (Relative Absolute Error)
	- RSE (Relative Squared Error)
	- R^2 = 1 - RSE
    2.5 Multiple Linear Regression
        : Predicting dependent variable with multiple independent variables
        2.5.1 Examples of multiple linear regression
	- Independent variables effectiveness on prediction
	- Predicting impacts of changes
	- ex. Co2Em = θ_0 + θ_1*Engine_size + θ_2*Cylinders + θ_3*Fuelconsumption + ...
	    → y^ = θ^T * X where θ^T = [θ_0  θ_1 θ_2  ... ], X = [1, x_1, x_2, ... ]
        2.5.2 Finding parameters
	- Using MSE (Mean Squared Error), Find minimun value of 1/n * ∑(y_i - y^_i)^2
	2.5.2.1 Ordinary Least Squares
	    - Lineaer algebra operations
	    - Takes a long time for large datasets (10K + rows)
	2.5.2.2 An optimization algorithm
	    - Gradient Descent
	    - Proper approach if you have a very large dataset
    2.6 Non-Linear Regression
        2.6.1 Polynomial regression is linear regression
	: A polynomial regression model can be transformed into linear regression model with mutiple linear regression (Least Squares)
        2.6.2 What is non-linear regression?
	- To model non-linear relationship between the dependent variable and a set of independent variables
	- y^ must be a non-linear function of the parameter θ, not necessarily the features x
        2.6.3 How can I know a non-linear problem?
	- Inspect visually, Based on accuracy
        2.6.4 How should I model non-linear data?
	- Polynomial regression
	- Non-linear regression model
	- Transform your data

3. Classification
    3.1 Introduction to classification
        3.1.1 What is classification??
	- A supervised learning approach
	- Categorizing some unknown items into a discrete set of categories or "classes"
	- The target attribute is a categorical variable
        3.1.2 Classification algorithms in machine learning
	- Decision Trees (ID3, C4.5, C5.0)
	- Naive Bayes
	- Linear Discriminant Analysis
	- k-Nearest neighbor
	- Logistic Regression
	- Neural Networks
	- Support Vector Machine (SVM)
    3.2 K-Nearest Neighbours
        3.2.1 What is K-Nearest Neighbor (or KNN)?
	- A method for classifying cases based on their similarity to other cases
        3.2.2 The K-Nearest Neighbors algorithm
	- 1. Pick a value for K
	- 2. Calculate the distance of unknown case from all cases
	- 3. Select the K-observations in the training data that are "nearest" to the unknown data point
	- 4. Predict the response of the unknown data point using the most popular response value from the K-nearest neighbors
        3.2.3 Evaluation Metrics in Classification
            3.2.3.1 Jaccard index
	  - Calculate the percentage of disjuction, ex. J(y, y^) = |y ∩ y^| / |y ∪ y^|
            3.2.3.2 F1-score
	  - Precision, Recall → F1-score
            3.2.3.3 Log loss
	  - LogLoss = -1/n ∑(y * log(y^) + (1 - y) * log(1 - y^))
    3.3 Decision Trees
        3.3.1 Decision tree algorithm
            - 1. Choose an attribute from your dataset
            - 2. Calcuate the significance of attribute in splitting of data
            - 3. Split data based on the value of the best attribute
            - 4. Go to step 1
        3.3.2 Building Decision Trees
            3.3.2.1 Which attribute is the best?
                - More Predictiveness is better attribute -> Find Less Impurity, Lower Entropy (High information gain)
                - Entropy, Measure of randomness or uncertainty -> The lower the Entropy, the less uniform the distribution, the purer the node
                - Entropy = - p(A)log(p(A)) - p(B)log(p(B))
                - Information gain, is the information that can increase the level of certainty after spliting
                - Information Gain = (Entropy before split) - (weighted entropy after split)
    3.4 Logistic Regression
        3.4.1 What is logistic regression?
	- Logistic regression is a classification algorithm for categorical variables (binary or multi classes classification)
	- It also can be used to predict probabilistic results with mapping the probabilities to 0 or 1
        3.4.2 When is logistic regression suitable?
	- If your data is binary (0/1, YES/NO, True/False)
	- If you need probabilistic results
	- When you need a linear decision boundary
	- If you need to understand the impact of a feature
        3.4.3 Logistic regression applications
	- Predicting the probability of a person having a heart attack
	- Predicting the mortality in injured patients
	- Predicting a customer's propensity to purchase a product or halt a subscription
	- Predicting the probability of failure of a given process or product
	- Predicting the likelihood of a homeowner defaulting on a mortgage

