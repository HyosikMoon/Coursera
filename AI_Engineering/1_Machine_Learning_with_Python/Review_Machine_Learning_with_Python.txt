1. Introduction to Machine Learning
    1.1 What is machine learning?
        : Machine learning is the subfield of computer science that gives "computers the ability to learn without being explicitly programmed."
    1.2 Major machine learning techniques
        1.2.1 Regression/Estimation: Predicting continuous values ✔
        1.2.2 Classification: Predicting the item class/category of a case ✔
        1.2.3 Clustering: Finding the structure of data; summarization ✔
        1.2.4 Associations: Associating frequent co-occuring items/events
        1.2.5 Anomaly detection: Discovering abnormal and unusual cases
        1.2.6 Sequence mining: Predicting next events; click-stream (Markov Model, HMM)
        1.2.7 Dimension Reduction: Reducing the size of data (PCA)
        1.2.8 Recomendation systems: Recommending items ✔
    1.3 Python for Machine Learning
        1.3.1 python libraries for machine learning
            1.3.1.1 NumPy
            1.3.1.2 SciPy
            1.3.1.3 matplotlib
            1.3.1.4 pandas
            1.3.1.5 scikit-learn
	    - Free software machine learning library
	    - Classification, Regression and Clustering algorithms
	    - Works with NumPy and SciPy
	    - Great docmentation
	    - Easy to implement
    1.4 Supervised vs Unsupervised
        1.4.1 What is supervised learning?
            : Teaching the model with labeled data
        1.4.2 Types of supervised learning
            - Regression: Regression is the process of predicting continuous values.
            - Classification: Classification is the process of predicting discrete class labels or categories.
        1.4.3 What is unsupervised learning?
	: The model works on its own to discover information
        1.4.4 Types of unsupervised learning
            - Dimension reduction
            - Density estimation
            - Market basket analysis
            - Clustering: Clustering is grouping of data points or objects that are somehow similar by discvoering structure, summarization and anomaly detection

2. Regression
    2.1 Introduction to Regression
        2.1.1 What is regression?
	: Regression is the process of predicting a continuous value(dependent variable) with independent variable(s)
        2.1.2 What is a regression model?
            - Simple regression model (predicting dependent variable with one independent variable)
            - Multiple regression model (predicting dependent variable with more than one independent variable)
        2.1.3 Applications of regression
            - Sales forecasting, Satisfaction analysis, Price estimation, Employment income
        2.1.4 Regression algorithms
            - Oridinal regression, Poisson regression, Fast forest quantile regression
            - Linear, Polynomial, Lasso, Stepwise, Ridge regression
            - Bayesian linear regression, Neural network regression, Decision forest regression, Boosted decision tree regression
            - KNN (K-nearest neighbors, classification algorithm but it can be used for regression too)
    2.2 Linear Regression 
        2.2.1 Simple Linear Regression 
            : Predicting dependent variable with one independent variable
        2.2.2 Multiple Linear Regression
            : Predicting dependent variable with multiple independent variables
            2.2.2.1 Examples of multiple linear regression
	  - Independent variables effectiveness on prediction
	  - Predicting impacts of changes
	  - ex. Co2Em = θ_0 + θ_1*Engine_size + θ_2*Cylinders + θ_3*Fuelconsumption + ...
	    → y^ = θ^T * X where θ^T = [θ_0  θ_1 θ_2  ... ], X = [1, x_1, x_2, ... ]
            2.2.2.2 Finding parameters
	  - Using MSE (Mean Squared Error), Find minimun value of 1/n * ∑(y_i - y^_i)^2
	  2.2.2.2.1 Ordinary Least Squares
	      - Lineaer algebra operations
	      - Takes a long time for large datasets (10K + rows)
	  2.2.2.2.2 An optimization algorithm
	      - Gradient Descent
	      - Proper approach if you have a very large dataset
    2.4 Non-Linear Regression
        2.4.1 Polynomial regression is linear regression
	: A polynomial regression model can be transformed into linear regression model with mutiple linear regression (Least Squares)
        2.4.2 What is non-linear regression?
	- To model non-linear relationship between the dependent variable and a set of independent variables
	- y^ must be a non-linear function of the parameter θ, not necessarily the features x
        2.4.3 How can I know a non-linear problem?
	- Inspect visually, Based on accuracy
        2.4.4 How should I model non-linear data?
	- Polynomial regression, Non-linear regression model, Transform your data
    2.5 Evaluation
        2.5.1 Model Evaluation in Regression Models
            2.5.1.1 Model evaluation approaches
                2.5.1.1.1 Train and Test on the same dataset
    	      - High training accuracy isn't necessarily a good thing
	      - Result of overf-fitting
                2.5.1.1.2 Train / Test Split
	      - It's important that our models have a high, out-of-sample accuracy
	      - More accuracte evaluation on out-of-sample accuracy
	      - Highly depedent on which datasets the date is trained and tested
            2.5.1.2 K-fold cross-validation
        2.5.2 Evaluation Metrics in regression models
            2.5.1.2 What is an error of the model?
	    - MAE (Mean Absolute Error), MSE (Mean Squared Error), RMSE (Root Mean Squared Error)
	    - RAE (Relative Absolute Error), RSE (Relative Squared Error)
	    - R^2 = 1 - RSE

3. Classification
    3.1 Introduction to classification
        3.1.1 What is classification??
	- A supervised learning approach
	- Categorizing some unknown items into a discrete set of categories or "classes"
	- The target attribute is a categorical variable
        3.1.2 Classification algorithms in machine learning
	- Decision Trees (ID3, C4.5, C5.0)
	- Naive Bayes
	- Linear Discriminant Analysis
	- k-Nearest neighbor
	- Logistic Regression
	- Neural Networks
	- Support Vector Machine (SVM)
    3.2 K-Nearest Neighbours
        3.2.1 What is K-Nearest Neighbor (or KNN)?
	- A method for classifying cases based on their similarity to other cases
        3.2.2 The K-Nearest Neighbors algorithm
	- 1. Pick a value for K
	- 2. Calculate the distance of unknown case from all cases
	- 3. Select the K-observations in the training data that are "nearest" to the unknown data point
	- 4. Predict the response of the unknown data point using the most popular response value from the K-nearest neighbors
        3.2.3 Evaluation Metrics in Classification
            3.2.3.1 Jaccard index
	  - Calculate the percentage of disjuction, ex. J(y, y^) = |y ∩ y^| / |y ∪ y^|
            3.2.3.2 F1-score
	  - Precision, Recall → F1-score
            3.2.3.3 Log loss
	  - LogLoss = -1/n ∑(y * log(y^) + (1 - y) * log(1 - y^))
    3.3 Decision Trees
        3.3.1 Decision tree algorithm
            - 1. Choose an attribute from your dataset
            - 2. Calcuate the significance of attribute in splitting of data
            - 3. Split data based on the value of the best attribute
            - 4. Go to step 1
        3.3.2 Building Decision Trees
            3.3.2.1 Which attribute is the best?
                - More Predictiveness is better attribute -> Find Less Impurity, Lower Entropy (High information gain)
                - Entropy, Measure of randomness or uncertainty -> The lower the Entropy, the less uniform the distribution, the purer the node
                - Entropy = - p(A)log(p(A)) - p(B)log(p(B))
                - Information gain, is the information that can increase the level of certainty after spliting
                - Information Gain = (Entropy before split) - (weighted entropy after split)
    3.4 Logistic Regression
        3.4.1 What is logistic regression?
	- Logistic regression is a classification algorithm for categorical variables (binary or multi classes classification)
	- It also can be used to predict probabilistic results with mapping the probabilities to 0 or 1
        3.4.2 When is logistic regression suitable?
	- If your data is binary (0/1, YES/NO, True/False)
	- If you need probabilistic results
	- When you need a linear decision boundary
	- If you need to understand the impact of a feature
        3.4.3 Logistic regression applications
	- Predicting the probability of a person having a heart attack
	- Predicting the mortality in injured patients
	- Predicting a customer's propensity to purchase a product or halt a subscription
	- Predicting the probability of failure of a given process or product
	- Predicting the likelihood of a homeowner defaulting on a mortgage
        3.4.4 Logistic Regression vs Linear Regression
	- Logistic Regression can measure the probability of a case beloning to a class but not with Linear Regression
	- Logistic Regression can predict the probability with Sigmoid function, σ(θ^T*X) = 1 / (1 + e^{-θ^T*X})
        3.4.5 Logistic Regression Training
            3.4.5.1 The training procee of logistic regression
                - 1. Initialize θ
                - 2. Calculate y^ = σ(θ^T*X) for a customer
                - 3. Compare the output of y^ with actual output of customer, y, and record it as error
                - 4. Calculate the error for all customers
                - 5. Change the θ to reduce the cost (ex. gradient descent method to find the proper θ)
                - 6. Go back to step 2
            3.4.5.2 Mminimizing the cost function of the model
                3.4.5.2.1 How to find the best parameters for our model?
	      - Minimize the cost function
                3.4.5.2.2 How to minimize the cost function?
	      - Using Gradient Descent
                3.4.5.2.3 What is gradient descent?
                    - A technique to use the derivative of a cost function to change the parameter values, in order to minimize the cost
            3.4.5.3 Training algorithm recap (with gradient descent metod)
                - 1. Initialize the parameters randomly (θ^T = [θ_0, θ_1, θ_2, ... ])
                - 2. Feed the cost function with training set, and calculate the error (J(θ) = 1/m∑[i=1..m](y^i * log(y^^i) + (1 - y^i) * log(1 - y^^i))
                - 3. Calculate the gradient of cost function (▽J = [δJ/δθ_1, δJ/δθ_2, ... , δJ/δθ_k])
                - 4. Update weights with new values (θ_new = θprev - η▽J)
                - 5. Go to step 2 until cost is small enough
                - 6. Predict the new customer X
    3.5 Support Vector Machine
        3.5.1 What is SVM?
            - SVM is a supervised algorithm that classfies cases by finding a separator
            - 1. Mapping data to a high-dimentional feature space
            - 2. Finding a separator
        3.5.2 Data transformation
            - Kernelling, Mapping data into a higher dimensional sapce, in such a way that can change a linearly inseperable dataset into a linearly separable dataset
            - Ways of Kernelling: Linear, Polynomial, RBF (Radio Bases Function), and Sigmoid
        3.5.3 Using SVM to find the hyperplane
            - Find the hyperlane by choosing Support vectors and maximize the margin
        3.5.4 Pros and cons of SVM
            3.5.4.1 Advantages
                - Accurate in high-dimentional spaces
                - Memory efficient
            3.5.4.2 Disadvantages
                - Prone to over-fitting
                - No probability estimation
                - Small datasets (Not efficient to large dataset, e.x. more than 1000 rows)
        3.5.5 SVM applications
            - Image recognition
            - Text category assignment
            - Detecting spam
            - Sentiment analysis
            - Gene Expression Classification
            - Regression, outlier detection and clustering

4. Clustering
    4.1 Introduction to Clustering
        4.1.1 What is clustering?
            - Cateogriging groups or objects having similar characteristics
            - What is cluster?, A group of objects that are similar to other objects in the cluster, and dissimilar to data points in other clusters
        4.1.2 Clustering vs Classification
            - Labled datatset -> Classification, Unlabled dataset -> Clustering
        4.1.3 Clustering applications
            4.1.3.1 Retail / Marketing, dentifying buying patterns of customers, Recommending new books or movies to new customers
            4.1.3.2 Banking, Fraud detection in credit card use, Identifying clusters of customers (e.g., loyal)
            4.1.3.3 Insurance, Fraud detection in claims analysis, Insurance risk of customers
            4.1.3.4 Publication, Auto-categorizing news based on their content, Recommending similar news articles
            4.1.3.5 Medicine, Characterizing patient behavior
            4.1.3.6 Biology, Clustering genetic markers to identify family ties
        4.1.4 General purpose
            - Exploratory data analysis, Summary generation, Outlier detection, Finding duplicates, Pre-processing step
        4.1.5 Clustering algorithms
            4.1.5.1 Partitioned-based Clustering, Relatively efficient, Middle, Large size dataset, e.g. k-Means, k-Median, Fuzzy c-Means
            4.1.5.2 Hierarchical Clustering, Produces trees of clusters, Small size dataset, e.g. Agglomerative, Divisive
            4.1.5.3 Density-based Clustering, Produces arbitrary shaped clusters, e.g. DBSCAN
    4.1 k-Means Clustering
        4.1.1 Characteristics of k-Means algorithms
            - Partitioning Clustering
            - K-means divides the data into non-overlapping subsets (clusters) without any cluster-internal structure
            - Examples within a cluster are very similar
            - Examples across different clusters are very different
        4.1.2 k-Means algorithms
            - 1. Initialize k centroids randomly
            - 2. Distance calculation from the centroid to each point
            - 3. Assign each point to the closest centroid
            - 4. Assign new centroids to the mean values of points in each cluster
            - 5. Repeat until there are no more changes (minimize the sum distance b/t centroids and each point in each cluster)
        4.1.3 k-Means accuracy
            4.1.3.1 External approach
                - Compare the clusters with the ground truth, if it is available
            4.1.3.2 Internal approach
                - Average the distance between data points within a cluster
    4.2 Hierarchical Clustering
        4.2.1 What is Hierarchical Clustering?
            - Hierarchical clustering algorithms build a hierarchy of clusters where each node is a cluster consists of the clusters of its daughter nodes
            - There are two types: Agglomerative(Bottom-up↑), Divisive(Top-down↓)
        4.2.2 Agglomerative algorithm
            - 1. Create n clusters, one for each data point
            - 2. Compute the Proximity Matrix
            - 3. Repeat
                - i. Merge the two closest clusters
                - ii. Update the proximity matrix
            - 4. Until only a single cluster remains
        4.2.3 Distance between clusters
            - Single-Linkage Clustering, Minimum distance between clusters
            - Complete-Linkage Clustering, Maximum distance between clusters
            - Average Linkage Clustering, Average distance between clusters
            - Centroid Linkage Clustering, Distance between cluster centroids
    4.3 Density-based Clustering
        4.3.1 What is DBSCAN?
            4.3.1.1 DBSCAN (Density-Based Spatial Clustering of Application with Noise)
                - Is one of the most common clustering algorithms
                - Works based on density of objects
            4.3.1.2 R (Radius of neighborhood)
                - Radius (R) that if includes enough number of points within, we call it a dense area
            4.3.1.3 M (Min number of neighbors)
                - The minimum number of data points we want in a neighborhood to define a cluster
        4.3.2 DBSCAN algorithm
            - 1. Check Core points/ Border points/ Outilers 
                 - Core point: A data point is a core point if within our neighborhood of the point there are at least M points
                 - Border points: A data point is a border point if its neighbourhood contains less than M data points or it is reachable from some core point
                 - Outliers: An outlier is a point that is not a core point and also is not close enough to be reachable from a core point
            - 2. Connect core points that are neighbors and put them in the same cluster
        4.3.3 Advantages of DBSCAN
            - 1. Aribitrary shaped clusters
            - 2. Robust to outliers
            - 3. Does not require specification of the number of clusters

5. Recommender Systems
    5.1 Intro to Recommender Systems
        5.1.1 What are recommender systems?
            - Recommender systems capture the pattern of peoples' behvior and use it to predict what else they ight want or like
        5.1.2 Two types of recommender systems
            5.1.2.1 Content-Based, Show me more of the same of whay I've liked before
            5.1.2.2 Collaborative Filtering, Tell me what's popular among my neighbors, I also might like it
        5.1.3 Implementing recommeder systems
            5.1.3.1 Memory-based (Statistical approach)
                - Uses the entire user-item dataset to generate a recommendation
                - Uses statistical techniques to approximate users or items
                - e.g., Pearson Correlation, Cosine Similarity, Euclidean Distance, etc
            5.1.3.2 Model-based
                - Develops a model of users in an attempt to learn their preferences
                - Models can be created using Machine Leraning techniques like regression, classification, clustering, etc
        5.1.4 Applications
            - What to buy?, E-commerce, books, movies, beer, shoes, etc.
            - Where to eat?, 
            - Which job to apply to?, LinkedIn
            - Who you should be firends with?, Facebook
            - Personalize your experience on the web, News platforms, news personalization
    5.2 Content-based Recommender Systems
        - ex. movies: [Input user ratings] x [Movies Matrix_feature sets] = [Normalized_Weighted Genre Matrix] x [New Movie List] ⇒ [Recommendation Matrix]
    5.3 Collaborative Filtering
        5.3.1 User-based collaborative filtering
            - Based on users' neighborhood
        5.3.2 Item-based collaborative filtering
            - Based on items' similarity
    5.4 Challenges of collaborative filtering
        5.4.1 Data Sparsity, Users in general rate only a limited number of items
        5.4.2 Cold start, Difficulty in recommendation to new users or new items
        5.4.3 Scalability, Increase in number of users or items (performance ↓ due to complexity of similarity computation)