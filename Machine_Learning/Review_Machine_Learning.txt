1. Introduction to Artificial Intelligence and Machine Learning
    1.1 What is AI, ML, DL ?
        - Artificial Intelligence:  Artificial Intelligence is a branch of computer science dealing with the simulation of intelligent behavior in computers.
        - Machine Learning:  Algorithms whose performance improve as they are exposed to more data over time
        - Deep Learning:  Subset of machine learning in which multilayered neural networks learn from vast amounts of data
    1.2 History of AI
        - AI has experienced cycles of AI winters and AI booms. 
        - AI solutions include speech recognition, computer vision, assisted medical diagnosis, robotics, and others.   
    1.3 Modern AI, Applications
        - Factors that have contributed to the current state of Machine Learning are: bigger data sets, faster computers, open source packages, and a wide range of neural network architectures. 
    1.4 Machine Learning Workflow ✔
        - Step1. Problem Statement → S2. Data collection → S3. Data Exploration & Data preprocessing (70~80%) → S4. Modeling → S5. Valiation → S6. Decision making & Development
    1.5 Machine Learning Vocabulary 
        - Target:  category or value you are trying to predict
        - Label:  the value of the target for a single data point
        - Features:  explanatory variables used for prediction
        - Examples:  an observation or single data point within the data
    1.6 Retrieving Data
        Retrieving data from multiple data sources such as SQL databases, NoSQL databases, APIs, and Cloud data sources
        1.6.1 CSV Files
            - Comma-seperated (CSV) files consists of rows of data, seperated by commas.
        1.6.2 JSON Files
            - JavaScript Object Notation (JSON) files are a standard way to store data across platforms.
        1.6.3 SQL Databases
            - Structured Query Language (SQL) represents a set of relational databases with fixed schemas.
            - ex. Microsoft SQL Server, Postgres, MySQL, AWS Redshift, Oracle DB, Db2 Family
        1.6.4 NoSQL Databases
            - Not-only SQL (NoSQL) databases are not relational, vary more in structure.
            - Most NoSQL databases store data in JSON format.
            - ex. Document databases (mongoDB, couchDB), Key-value stores (Riak, Voldemort, Redis), Graph databases (Neo4j, HyperGraph), Wide-column stores (Cassandra, HBase)
        1.6.5 APIs and Cloud Data Access
            - A variety of data providers make data available via Application Programming Interfaces (APIs), that makes it easy to access such data via Python.
    1.6 Data Cleaning ✔ Step3
        - Data cleaning is one of ways to preprocess data among data Cleaning, Integration, Reduction, and Transformation
        - Key aspects of Machine Learning Workflow depend on cleaned data: Observations, Labels, Algorithms, Features, Model
          (Algorithms: Computer programs that estimate models based on available data, Model: Hypothesized relationship between observations and data)
        - For missing data → Remove, Impute, Mask.  /  For outliers → Remove, Assign, Transform, Predict, Keep.
    1.7 Exploratory Data Analysis (EDA) ✔ Step3
        - Exploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods.
        - Visualization can be created in multiple ways such as Matplotlib, Pandas (via Matplotlib), and Seaborn
    1.8 Feature Engineering and Variable Transformation ✔ Step3
        Variables must often be transformed before they can be included in models. ex. log and polynomial transformations, Encoding (non-numeric → numeric), Scaling
        1.8.1 Feature encoding
            - Binary encoding, One-hot encoding, Ordinal encoding
        1.8.2 Feature scaling
            - Standard scaling, Min-max scaling, Robust scaling
            - ex. Log transforming skew variables
    1.9 Estimation and Inference
        1.9.1 Esitimation:  Estimation is the application of an algorithm, for example taking an average
        1.9.2 Inference:  Inference involves putting an accuracy on the estimate (e.g. standard error of an average, confidence interval)        
    1.10 Hypothesis Testing
        - Hypothesis testing is an act in statistics whereby an analyst tests an assumption regarding a population parameter.
        - Hypothesis testing is used to assess the plausibility of a hypothesis by using sample data.
        - In hypothesis testing, an analyst tests a statistical sample, with the goal of providing evidence on the plausibility of the null hypothesis.
        - significance leve, p-value, ex. https://www.youtube.com/watch?v=KS6KEWaoOOE

2. Supervised Machine Learning: Regression
    2.1 Introduction to Supervised Machine Learning and Linear Regression
        2.1.1 Types of Machine Learning
            - y_p = f(Ω, x)  where x=input, y_p=output(values predicted by the model), Ω=parameters
            - J(y, y_p): Loss, Machine Learning model chooses parameters Ω to minimize loss J by using features x and outcome y.
            - cf. hyperparameters is a parameter that is not learned directly from the data, but relates to implementation of training our ML model.
            - ex. Regression's output(y) is numeric, and Classification's output(y) is categorical.
        2.1.2 Interpretation and Prediction
            - Interpretation is to train a model to find insights from the data especiall from the Ω.
            - Prediction is to make the best prediction comparing y_p with y.
        2.1.3 Linear Regression 
            - Making our target variable normally distributed often will lead to better results 
              → Check normality by visually and using a statistical data (ex. from scipy.stats.mstats import normaltest)
            - Try some common transformations to try and get y to be normally distributed:  ex. Log, Square root, Box cox
            2.1.3.1 Process of Linear Regression ✔ Step4. Modelling
                - 1) Create X and Y
                - 2) Polynomial Features of X 		→	X_pf = pf.fit_transform(X)  where, pf = PolynomialFeatures(degree=2, include_bias=False)
                - 3) Train test split of X_pf, Y 		→ 	X_train, X_teest, Y_train, Y_test = train_test_split(X_pf, Y, test_size=0.3, random_state=42)
                - 4) Standard Scalar of X_train 		→ 	X_train_s = s.fit_transform(X_train)  where, s = StandardScaler()
                - 5) normalize Y_train 			→ 	Y_train_bc = boxcox(Y_train)[0], lambda1 = boxcox(Y_train)[1]
                - 6) fit model 				→ 	lr(X_train_s, Y_train)
                - 7) Standard Scalar of X_test 		→ 	X_test_s = s.transform(X_test)
                - 8) Predict normalized Y_pred 		→ 	Y_pred_bc = lr.predict(X_test_s)
                - 9) Inverse of Y_pred_bc			→	Y_pred = inv_boxcox(Y_pred_bc, lambda1) 
                - 10) Determine R^2 b/w Y_test and Y_pred	→	r2_score(y_pred, y_test)  where, from sklearn.metrics import r2_score
    2.2 Data Splits and Cross Validation
    2.3 Regression with Regularization Techniques: Ridge, LASSO, and Elastic Net





3. Supervised Machine Learning: Classification
4. Unsupervised Machine Learning
5. Deep Learning and Reinforcement Learning
6. Specialized Models: Time Series and Survival Analysis