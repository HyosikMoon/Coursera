1. Introduction to Artificial Intelligence and Machine Learning
    1.1 What is AI, ML, DL ?
        - Artificial Intelligence:  Artificial Intelligence is a branch of computer science dealing with the simulation of intelligent behavior in computers.
        - Machine Learning:  Algorithms whose performance improve as they are exposed to more data over time
        - Deep Learning:  Subset of machine learning in which multilayered neural networks learn from vast amounts of data
    1.2 History of AI
        - AI has experienced cycles of AI winters and AI booms. 
        - AI solutions include speech recognition, computer vision, assisted medical diagnosis, robotics, and others.   
    1.3 Modern AI, Applications
        - Factors that have contributed to the current state of Machine Learning are: bigger data sets, faster computers, open source packages, and a wide range of neural network architectures. 
    1.4 Machine Learning Workflow ✔
        - Step1. Problem Statement → S2. Data collection → S3. Data Exploration & Data preprocessing (70~80%) → S4. Modeling → S5. Valiation → S6. Decision making & Development
    1.5 Machine Learning Vocabulary 
        - Target:  category or value you are trying to predict
        - Label:  the value of the target for a single data point
        - Features:  explanatory variables used for prediction
        - Examples:  an observation or single data point within the data
    1.6 Retrieving Data
        Retrieving data from multiple data sources such as SQL databases, NoSQL databases, APIs, and Cloud data sources
        1.6.1 CSV Files
            - Comma-seperated (CSV) files consists of rows of data, seperated by commas.
        1.6.2 JSON Files
            - JavaScript Object Notation (JSON) files are a standard way to store data across platforms.
        1.6.3 SQL Databases
            - Structured Query Language (SQL) represents a set of relational databases with fixed schemas.
            - ex. Microsoft SQL Server, Postgres, MySQL, AWS Redshift, Oracle DB, Db2 Family
        1.6.4 NoSQL Databases
            - Not-only SQL (NoSQL) databases are not relational, vary more in structure.
            - Most NoSQL databases store data in JSON format.
            - ex. Document databases (mongoDB, couchDB), Key-value stores (Riak, Voldemort, Redis), Graph databases (Neo4j, HyperGraph), Wide-column stores (Cassandra, HBase)
        1.6.5 APIs and Cloud Data Access
            - A variety of data providers make data available via Application Programming Interfaces (APIs), that makes it easy to access such data via Python.
    1.6 Data Cleaning ✔ Step3
        - Data cleaning is one of ways to preprocess data among data Cleaning, Integration, Reduction, and Transformation
        - Key aspects of Machine Learning Workflow depend on cleaned data: Observations, Labels, Algorithms, Features, Model
          (Algorithms: Computer programs that estimate models based on available data, Model: Hypothesized relationship between observations and data)
        - For missing data → Remove, Impute, Mask.  /  For outliers → Remove, Assign, Transform, Predict, Keep.
    1.7 Exploratory Data Analysis (EDA) ✔ Step3
        - Exploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods.
        - Visualization can be created in multiple ways such as Matplotlib, Pandas (via Matplotlib), and Seaborn
    1.8 Feature Engineering and Variable Transformation ✔ Step3
        Variables must often be transformed before they can be included in models. ex. log and polynomial transformations, Encoding (non-numeric → numeric), Scaling
        1.8.1 Feature encoding
            - Binary encoding, One-hot encoding, Ordinal encoding
        1.8.2 Feature scaling
            - Standard scaling, Min-max scaling, Robust scaling
            - ex. Log transforming skew variables
    1.9 Estimation and Inference
        1.9.1 Esitimation:  Estimation is the application of an algorithm, for example taking an average
        1.9.2 Inference:  Inference involves putting an accuracy on the estimate (e.g. standard error of an average, confidence interval)        
    1.10 Hypothesis Testing
        - Hypothesis testing is an act in statistics whereby an analyst tests an assumption regarding a population parameter.
        - Hypothesis testing is used to assess the plausibility of a hypothesis by using sample data.
        - In hypothesis testing, an analyst tests a statistical sample, with the goal of providing evidence on the plausibility of the null hypothesis.
        - significance leve, p-value, ex. https://www.youtube.com/watch?v=KS6KEWaoOOE

2. Supervised Machine Learning: Regression
    - Regression models, which predict a continuous outcome
    2.1 Introduction to Supervised Machine Learning and Linear Regression
        2.1.1 Types of Machine Learning
            - y_p = f(Ω, x)  where x=input, y_p=output(values predicted by the model), Ω=parameters
            - J(y, y_p): Loss, Machine Learning model chooses parameters Ω to minimize loss J by using features x and outcom e y.
            - cf. hyperparameters is a parameter that is not learned directly from the data, but relates to implementation of training our ML model.
            - ex. Regression's output(y) is numeric, and Classification's output(y) is categorical.
        2.1.2 Interpretation and Prediction
            - Interpretation is to train a model to find insights from the data especiall from the Ω.
            - Prediction is to make the best prediction comparing y_p with y.
        2.1.3 Linear Regression 
            - Making our target variable normally distributed often will lead to better results 
              → Check normality by visually and using a statistical data (ex. from scipy.stats.mstats import normaltest)
            - Try some common transformations to try and get y to be normally distributed:  ex. Log, Square root, Box cox
            2.1.3.1 Process of Linear Regression ✔ Step4. Modelling
                - 1) Create X and Y
                - 2) Polynomial Features of X 		→	X_pf = pf.fit_transform(X)  where, pf = PolynomialFeatures(degree=2, include_bias=False)
                - 3) Train test split of X_pf, Y 		→ 	X_train, X_teest, Y_train, Y_test = train_test_split(X_pf, Y, test_size=0.3, random_state=42)
                - 4) Standard Scalar of X_train 		→ 	X_train_s = s.fit_transform(X_train)  where, s = StandardScaler()
                - 5) normalize Y_train 			→ 	Y_train_bc = boxcox(Y_train)[0], lambda1 = boxcox(Y_train)[1]
                - 6) fit model 				→ 	lr(X_train_s, Y_train)
                - 7) Standard Scalar of X_test 		→ 	X_test_s = s.transform(X_test)
                - 8) Predict normalized Y_pred 		→ 	Y_pred_bc = lr.predict(X_test_s)
                - 9) Inverse of Y_pred_bc			→	Y_pred = inv_boxcox(Y_pred_bc, lambda1) 
                - 10) Determine R^2 b/w Y_test and Y_pred	→	r2_score(y_pred, y_test)  where, from sklearn.metrics import r2_score
            2.1.3.2 One-hot-encoding for categorical variables
            2.1.3.3 Scaling variables for linear regression
                - Scaling doesn't have an effect on the plain-linear regression result
            2.1.3.4 Visualize the result (ex. Scatter plot)
    2.2 Data Splits and Cross Validation ✔ Step5. Validation
        2.2.1 Training and Test Splits
            - Splitting your data into a training and a test set can help you choose a model that has better chances at generalizing and is not overfitted.
            - ex. X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.3, random_state=42)
        2.2.2 Cross Validation
            - Find the best model of a complexity where training and cross validation errors are both low
            - Training error tends to decrease with a more complex model. 
            - Cross validation error generally has a u-shape. It decreases with more complex models, up to a point in which it starts to increase again. 
            2.2.2.1 K-fold Cross Validation:  Using each of k subsamples as a test sample
                - ex. kf = KFold(shuffle=True, random_state=72018, n_splits=3)
            2.2.2.2 Leave One Out Cross Validation:  Using each observation as a test sample
            2.2.2.3 Stratified Cross Validation:  K-fold Cross Validation with representative samples
        2.2.3 ex. LASSO (Least Absolute Shrinkage and Selection Operator)
            - ex1. las = Lasso(alpha=best_alpha, max_iter=100000)
                     estimator = Pipeline([("make_higher_features", pf), ("scaler", s), ("lasso_regression", las)])  #where, s = StandardScaler()
                     predictions = cross_val_predict(estimator, X, y, cv=kf)  #where, kf = KFold(shuffle=True, random_state=72018, n_splits=3)
                     score = r2_score(y, predictions)
            - ex2. best_estimator = Pipeline([("make_higher_degree", PolynomialFeatures(degree=2, include_bias=False)), 
				  ("scaler", s), ("lasso_regression", Lasso(alpha=best_alpha, max_iter=100000))])
	       best_estimator.fit(X, y)
	       best_estimator.score(X, y)
        2.2.4 Hyperparameter selection
            - Use GridSearchCV
            - ex. from sklearn.model_selection import GridSearchCV
	     estimator = Pipeline([  ("polynomial_features",  PolynomialFeatures()),  ("scaler", StandardScaler()),  ("ridge_regression", Ridge())  ])
	     params = {  'polynomial_features__degree': [1, 2, 3],  'ridge_regression__alpha': np.geomspace(4, 20, 30)  }
	     grid = GridSearchCV(estimator, params, cv=kf)
	     grid.fit(X, y)
	     y_predict = grid.predict(X)
	     r2_score(y, y_predict)
	     # grid.best_score_, grid.best_params_
	     # grid.best_estimator_.named_steps['ridge_regression'].coef_
    2.3 Regularization Techniques: Ridge, LASSO, and Elastic Net ✔ Step5. Validation, Step6. Decision making
        2.3.1 Bias and Variance
            - Bias is a tendency to miss. ex. Underfitting (High bias, Low variance), low-order function
            - Variance is tendency to be inconsistent  ex. Overfitting (Low bias, High variance), high-order function
        2.3.2 Regularization and Feature selection
            - M(w) + λR(w) : Adjusted cost function  where M(w): model error, R(w): function of extimated parameters, λ: regularization strength parameter
            - This λ (lambda) adds a penalty proportional to the size of the estimated model parameter, or a function of the parameter.
            - Regularization performs feature selection by shrinking the contribution of features.
            - Feature selection can also be performed by removing features.
        2.3.3 LASSO, Ridge regression and Elastic Net 
            - When train LASSO, Ridge, and Elastic Net, it's a best practice to scale fetures so penalties aren't impacted by variable scale.
            - In LASSO regression, the complexity penalty λ (lambda) is proportional to the absolute value of coefficients. (L1-abs, L2-squared)
            - In Ridge regression, the complexity penalty λ (lambda) is applied proportionally to squared coefficient values.
            - Elastic Net, an alternative hybrid approach, introduces a new parameter α(alpha) that determines a weighted average of L1 and L2 penalties.
        2.3.4 Recursive Feature Elimination (REF)
            - RFE applies the model, measures feature importance, and recursively removes less important features to the desired number of features.
        2.3.5 Details of Regularization 
            - Regularization techniques have an analytical, a geometric, and a probabilistic interpretation.  
    2.4 Polynomial Regression
        - "Linear Regression" means linear combinations of features not linear relationship between features(variables)
        - ex. y(x) = β0 + β1*x + β2*x^2 + β3*x^3   or   y(x) = β0 + β1*x1 + β2*x2 + β3*x1*x2  are also linear regression models.
        - Polynomial terms help you capture nonlinear effects of your features. (Capture the relation of the outcome with features of higher order.)
    *Regression Summary: 2.1 Modeling, 2.2 Validation (Cross validation, Regularization) 2.3 Decision (Error rates, Coefficients analysis)

3. Supervised Machine Learning: Classification
    - Classification models, which predict a categorical outcome.
    3.1 Logistic Regression
        3.1.1 What is Logistic Regression
            - Logistic regression is a type of regression that models the probability of a certain class occurring given other independent variables.
            - The input of logistic regression result is the linear regression, and the output is the probability belonging to a specific category. 
            - from sklearn.linear_model import LogisticRegression
            - from sklearn.linear_model import LogisticRegressionCV
            - from sklearn.metrics import precision_recall_fscore_support as score
        3.1.2 Classification Error Metrics
            3.1.2.1 Confusion matrix:  A confusion matrix tabulates true positives, false negatives, false positives and true negatives.
            3.1.2.2 ROC curve:  The receiver operating characteristic (ROC) plots the true positive rate (sensitivity) of a model vs. its false positive rate (1-specificity).
            3.1.2.3 Precision-Recall curve:  The precision-recall curve measures the trade-off between precision and recall.
            - The ROC curve generally works better for data with balanced classes, while the precision-recall curve generally works better for data with unbalanced classes.  
    3.2 K Nearest Neighbors
            - K nearest neighbor methods are useful for classification. The elbow method is frequently used to identify a model with low K and low error rate.
              # Categorical(nominal (binary: only two states), ordinal), Quantitative(Numeric (interval, ratio))
              # Categorical(ordinal) -> LabelEncoder -> MinMaxScaler
              # Categorical(nominal) -> OneHotEncoder or get_dummies
              # Categorical(binary)  -> LabelBinarizer
              # Numeric -> MinMaxScaler
            - from sklearn.neighbors import KNeighborsClassifier
              knn = KNeighborsClassifier(n_neighbors=5, weights='distance')
              knn = knn.fit(X_train, y_train)
              y_pred = knn.predict(X_test)
            - from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, f1_score
    3.3 Support Vector Machines
        - Regularization in SVMs, cost function, J(βi) = SVMCost(βi) + 1/c ∑iβi
        - from sklearn.svm import LinearSVC
        - from sklearn.svm import SVC	# Gaussian Kernel		# g, c↓ → means higher regularization = complexity↓ 
          SVC_Gaussian = SVC(kernel='rbf', gamma=g, C=c)    	# g, c↑ → means lower regularization = complexity↑
    3.4 Decision Trees
        - Decision trees split your data using impurity measures such as Entropy and Gini index. (Greedy algorithm)
        - Decision trees tend to overfit and to be very sensitive to different data. Cross validation and pruning sometimes help with some of this.
        - Great advantages of decision trees are that they are really easy to interpret and require no data preprocessing.
        3.4.1 DecisionTreeClassfier
            - from sklearn.tree import DecisionTreeClassifier
              from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
            - from sklearn.model_selection import GridSearchCV
        3.4.2 DecisionTreeRegressor
            - from sklearn.tree import DecisionTreeRegressor
              from sklearn.metrics import mean_squared_error
    3.5 Ensemble Models
        3.5.1 Bagging (Boostrap aggregating)
            - Pick Sample / Average
            - A model that averages the predictions of multiple models reduces the variance of a single model and has high chances to generalize well.
            - Bagging is a tree ensemble that combines the prediction of several trees that were trained on bootstrap samples of the data.
            - from sklearn.ensemble import BaggingClassifier
        3.5.2 RandomForest
            - Pick Sample / Pick Features / Average
            - Bagging & Choose features randomly
            - from sklearn.ensemble import RandomForestClassifer
        3.5.3 Extra RandomForest
            - RandomForest & Create splits randomly (not greedily)
            - from sklearn.ensemble import ExtraTreesClassifier
        3.5.4 Boosting
            - Advance residually
            - Boosting methods are additive in the sense that they sequentially retrain decision trees using the observations with the highest residuals on the previous tree. 
            - To do so, observations with a high residual are assigned a higher weight.
            - The additive nature of gradient boosting makes it prone to overfitting. This can be addressed using cross validation, boosting iterations, and hyperparameters. 
            3.5.4.1 Loss functions for boosting algorithms
                3.5.4.1.1 0-1 loss function, which ignores observations that were correctly classified. The shape of this loss function makes it difficult to optimize.
                3.5.4.1.2 Adaptive boosting loss function, which has an exponential nature. The shape of this function is more sensitive to outliers.
                3.5.4.1.2 Gradient boosting loss function. The most common gradient boosting implementation uses a binomial log likelihood loss function called deviance. 
            - from sklearn.ensemble import GradientBoostingClassifier
            - from sklearn.ensemble import AdaBoostClassifier
        3.5.6 Stacking
            - Ensemble with different models
            - Stacking is an ensemble method that combines any type of model by combining the predicted probabilities of classes.
            - The two most common ways to combine the predicted probabilities are using a majority vote or using weights for each predicted probability.  
            - from sklearn.ensemble import VotingClassifier
        3.5.7 Evaluation
            - from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score
            - from sklearn.metrics import f1_score, roc_auc_score
            - from sklearn.metrics import roc_curve, precision_recall_curve, confusion_matrix
    3.6 Modeling Unbalanced Classes
        - https://imbalanced-learn.org/stable/user_guide.html (scikit-learn unbalanced classes modeling guide)
        - Ensure that you first do your train test split before doing any of this over or undersampling. (To decrease the overfitting)
        - Not use accuracy to evaluate unbalanced data set but to use f1-score, ROC, precision-recall curves 
        3.6.1 Downsampling
            - Downsampling or removing observations from the most common class
            - Recall goes up, and precision will go down  
        3.6.2 Upsampling
            - Upsampling or duplicating observations from the rarest class or classes
            - Recall goes up, and precision will likely go down, but the gap is lesser. 
        3.6.3 Resampling
            - A mix of downsampling and upsampling. Upsampling the minority class, and Downsampling the majority class.
        3.6.4 Weighting, Stratified Sampling
        3.6.5 Oversampling
            3.6.5.1 Random Oversampling (Randomly resample with replacement from minority class)
            3.6.5.2 Synthetic Oversampling 
                3.6.5.2.1 SMOTE - Synthetic Minority Oversampling Techniuque
                    - Regular (Connect minority class points to any neighbor → Synthesize new minority between two points)
                    - Borderline (Classify points as outlier, safe, or in-danger)
                    - SVM (Use minority support vectors to generate new points)
                3.6.5.2.2 ADASYN - ADAptive SYNthetic sampling
        3.6.6 Undersampling
            3.6.5.1 NearMiss
                3.6.5.1.1 NearMiss-1 (Keep majority points closest to Nearby Minority Points)
                3.6.5.1.2 NearMiss-2 (Keep majority points closest to Distant Minority Points)
                3.6.5.1.2 NearMiss-3 (Keep majority points closest to Majority Neighbors of Minority Points)
            3.6.5.2 Tomek Links (Mixed Mutual Nearest Neighbors) 
                - Remove majority or minority points both closest neighbors together
            3.6.5.3 Edited Nearest neighbors (Remove points that don't agree with neighbors' boundaries)
                 
4. Unsupervised Machine Learning
    4.1 Introduction to Unsupervised Learning and Types
        - Unsupervised algorithms are relevant when we don’t have an outcome or labeled variable we are trying to predict.
        4.1.1 Clustering
                - K-Means
                - HAC (Hierarchical Agglomerative Clustering), try to continuously (split out and) merge new clusters successively until it reaches a level of convergence. 
                - DBSCAN
                - Mean Shift, partitioning algorithm that assigns to nearest cluster centroid.
        4.1.2 Dimensionality Reduction
                - PCA (Principal Components Analysis)
                - Non-negative Matrix Factorization
    4.2 Selecting a clustering algorithm
        4.2.1 Distance Metrics
            - Euclidean Distance, Manhattan Distance, Cosine Distance, Jaccard Distance
        4.2.1 Linkage Methods (Hierarchical Clustering)
            - Single linkage, Complete linkage, Average linkage, Ward linkage (Cluster merge is based on inertia)
    4.3 Dimensionality Reduction
        4.3.1 Principal Components Analysis
            - Identify small number of transformed variables with different effects, preserving variance
        4.3.2 KernalPCA
            - Useful for situations with nonlinear relationships, but requires more computation than PCA
        4.3.3 Non-negative Matrix Factorization
            - Useful when you want to consider only positive values (word matrics, images)

5. Deep Learning and Reinforcement Learning
    5.1 Introduction to Neural Networks
        5.1.1 Deep Learning Model Summary
            5.1.1.1 Neural Network Models (Multi-Layer Perceptron, Feedforward Networks)
                - Applied to many traditional predictive porblems (classification and regression, tabular data)
                - ex. from sklearn.neural_network import MLPClassifier  #(Multi-Layer Perceptron Syntax)
            5.1.1.2 Recurrent Neural Networks (RNN, LSTM)
                - Useful for modeling sequences (time-series forecasting, sentence prediction)
            5.1.1.3 Convolutional Neural Networks (CNN)
                - Useful for feature and object recognition in visual data (images, video). Also applied in other contexts (forecasting)
            5.1.1.4 Unsupervised Pre-trained Networks (Autoencoders, Deep Belief Networks, and Generative Adversarial Networks)
                - Many uses including generating images, labeling outcomes, dimensionality reduction
        5.1.2 Optimization and Gradient Descent
            - Gradient descent, (Full-batch gradient descent, Vanilla gradient descent) (Entire data)
            - Stochastic gradient descent (Single data)
            - Mini-batch gradient descent (n numbers of data)  
        5.1.3 Backpropagation
        5.1.4 Activation Functions
            5.1.4.1 Sigmoid Activation
                - Useful when outcomes should be in (0, 1), suffers from vanishing gradient issues
            5.1.4.2 Hyperbolic Tangent
                - Useful when outcomes should be in (-1, 1), suffers from vanishing gradient issues
            5.1.4.3 ReLU (Rectified Linear Unit)
                - Useful to capture large effects, doesn't suffer from vanishing gradient
            5.1.4.4 LReLU ("Leaky" Rectified Linear Unit)
                - Acts like ReLU, but allows negative outcomes
        5.1.5 Regularization
            5.1.5.1 Regularization penalty in cost function
            5.1.5.2 Dropout
            5.1.5.3 Early stopping
            5.1.5.4 Stochastic / Mini-batch Gradient descent
    5.2 Neural Network Optimizers and Keras
        5.2.1 Optimizers and Data Shuffling
            5.2.1.1 Optimizers
                - This approaches are based on the idea of tweaking and improving the weights using other methods instead of gradient descent.
                - There are several variants to updating the weights which give better performance in practice.  (ex. Momentum, RMSProp, Adam, etc)
            5.2.1.2 Data Shuffling
                - Data shuffling, which aids convergence by making sure data is presented in a different order every epoch.
        5.2.2 Keras Library
            5.2.2.1 Typcial command structure
                - 1) Compile the model, specifying your loss function, metrics, and optimizer
                - 2) Fit the model on your training data (specifying batch size, number of epochs)
                - 3) Predict on new data
                - 4) Evaluate your results
            5.2.2.2 Building the model
                - Sequential Model:  allows a linear stack of layers, simpler and more convenient if model has this form
                - Functional API:  more detailed and complex, but allows more complicated architectures  (ex. Autoencoder, Variational autoencoder)
                * import pickle 		                          	# Write GV_GBC object as an pickle file, so we can load it later.
                  pickle.dump(GV_GBC, open('gv_gbc.p', 'wb'))  	#write byte
                  GV_GBC = pickle.load(open('gv_gbc.p', 'rb'))   	#read byte
    5.3 Convolutional Neural Networks
        5.3.1 Inroduction to CNN
        5.3.2 CNN Settings (Padding, Stride, Depth, Pooling)
            - Padding,  adding extra pixels 0 to the boundary of the input image
            - Stride,  "step size" as the kernel moves across the image
            - Depth,  the number of channel is referred to as the "depth". ex. RGB image has 3 channels (depth)
            - Pooling,  reduce the image size by mapping a patch of pixels to a single value
        5.3.3 Transfer Learning
            - The main idea of Transfer Learning consists of keeping early layers of a pre-trained network and re-train the later layers for a specific application.
            - Last layers in the network capture features that are more particular to the specific data you are trying to classify.
            - Later layers are easier to train as adjusting their weights has a more immediate impact on the final result.
        5.3.4 CNN Architectures
            5.3.4.1 LeNet-5
                - Created by Yann LeCun in the 1990s, and Used on the MNIST data set.
                - Novel Idea: Use convolutions to efficiently learn features on data set.
            5.3.4.2 AlexNet
                - Considered the “flash point” for modern deep learning, Created in 2012 for the ImageNet Large Scale Visual Recognition Challenge (ILSVRC).
                - AlexNet developers performed data augmentation for training. (ex. Cropping, horizontal flipping, and other manipulations.)
            5.3.4.3 VGG
                - Simplify Network Structure: has same concepts and ideas from LeNet, considerably deeper.
                - This was one of the first architectures to experiment with many layers (More is better!). 
                - It can use multiple 3x3 convolutions to simulate larger kernels with fewer parameters and it served as ”base model” for future works.
            5.3.4.4 Inception
                - This architecture was built to turn each layer of the neural network into further branches of convolutions. Each branch handles a smaller portion of workload.
                - The network concatenates different branches at the end. 
            5.3.4.5 ResNet
                - In the very deep (56-layer) networks, the early layers were just not getting updated and the signal got lost (due to vanishing gradient type issues).
                - In theory, should be able to just have an “identity” transformation that makes the deeper network behave like a shallow one.
                - Keeps passing both the the initial unchanged information and the transformed information to the next layer.
    5.4 Recurrent Neural Networks and Long-Short Term Memory Networks
        5.4.1 Recurrent Neural Networks
            - Recurrent Neural Networks are a class of neural networks that allow previous outputs to be used as inputs while having hidden states.
            - current_state = function1(old state, curren_input),  current_output = function2(current_state)
            - We learn function1 and function2 by training our network
        5.4.2 Long-Short Term Memory networks (LSTMs)
            - LSTM solves one of the main weaknesses of RNNs, which makes it hard to keep information from distant past in current memory without reinforcement.
            - Leave some dimensions unchanged over many steps.
        5.4.3 Gated Recurrent Units (GRUs)
            - GRUs are a gating mechanism for RNNs that is an alternative to LSTM. It is based on the principle of Removed Cell State.
            - Think of as a “simpler” and faster version of LSTM.
        5.4.4 Sequence-to-Sequence Models (Seq2Seq)
            - Seq2Seq models improve accuracy by keeping necessary information in the hidden state from one sequence to the next.
            - This way, at the end of a sentence, the hidden state will have all information relating to past words. 
        5.4.5 Beam Search
            - Beam search tries to produce multiple different hypotheses to produce words until <EOS> and then see which full sentence is most likely.
    5.5 Deep Learning with Autoencoders
        5.5.1 Autoencoders
            - Autoencoders learn a compressed representation of the input by first compressing the input (encoding) and decompressing it back (decoding) to match the original input.
        5.5.2 Autoencoding process
            - 1) Feed image through encoder network
            - 2) Generate the lower dimension embedding
            - 3) Feed embedding through decoder network
            - 4) Generate reconstructed version of the original data
            - 5) Compare the result of the generated vs the original data
            - Result: A network will learn the lower dimensional space that represents the original data.
        5.5.3 Variational Autoencoders
            - Variational autoencoders also generate a latent representation and then use this representation to generate new samples (i.e. images).
            - Data are assumed to be represented by a set of normally-distributed latent factors.
            - The encoder generates parameters of these distributions, namely µ and σ. Images can be generated by sampling from these distributions. 
    5.6 Deep Learning Applications and Reinforcement Learning
        5.6.1 Generative Adversarial Networks
            - GANs are way of training two neural networks simultaneously.
            - One of the neural networks, the 'generator', learns to map random noise to images indistinguishable from those in some training set.
            - The other neural network, the 'discriminator', decides for which images are generated or are fake and which ones are the actual images from our training set.
        5.6.2 Reinforcement Learning
            - Agent → Actions → Environment → Rewards → Agent
            - 1) Gather data, 2) Allocate rewards, 3) Fit the model, 4) Predict the optimal results with the maximal reward inputs

6. Specialized Models: Time Series and Survival Analysis