1. Introduction to Artificial Intelligence and Machine Learning
    1.1 What is AI, ML, DL ?
        - Artificial Intelligence:  Artificial Intelligence is a branch of computer science dealing with the simulation of intelligent behavior in computers.
        - Machine Learning:  Algorithms whose performance improve as they are exposed to more data over time
        - Deep Learning:  Subset of machine learning in which multilayered neural networks learn from vast amounts of data
    1.2 History of AI
        - AI has experienced cycles of AI winters and AI booms. 
        - AI solutions include speech recognition, computer vision, assisted medical diagnosis, robotics, and others.   
    1.3 Modern AI, Applications
        - Factors that have contributed to the current state of Machine Learning are: bigger data sets, faster computers, open source packages, and a wide range of neural network architectures. 
    1.4 Machine Learning Workflow ✔
        - Step1. Problem Statement → S2. Data collection → S3. Data Exploration & Data preprocessing (70~80%) → S4. Modeling → S5. Valiation → S6. Decision making & Development
    1.5 Machine Learning Vocabulary 
        - Target:  category or value you are trying to predict
        - Label:  the value of the target for a single data point
        - Features:  explanatory variables used for prediction
        - Examples:  an observation or single data point within the data
    1.6 Retrieving Data
        Retrieving data from multiple data sources such as SQL databases, NoSQL databases, APIs, and Cloud data sources
        1.6.1 CSV Files
            - Comma-seperated (CSV) files consists of rows of data, seperated by commas.
        1.6.2 JSON Files
            - JavaScript Object Notation (JSON) files are a standard way to store data across platforms.
        1.6.3 SQL Databases
            - Structured Query Language (SQL) represents a set of relational databases with fixed schemas.
            - ex. Microsoft SQL Server, Postgres, MySQL, AWS Redshift, Oracle DB, Db2 Family
        1.6.4 NoSQL Databases
            - Not-only SQL (NoSQL) databases are not relational, vary more in structure.
            - Most NoSQL databases store data in JSON format.
            - ex. Document databases (mongoDB, couchDB), Key-value stores (Riak, Voldemort, Redis), Graph databases (Neo4j, HyperGraph), Wide-column stores (Cassandra, HBase)
        1.6.5 APIs and Cloud Data Access
            - A variety of data providers make data available via Application Programming Interfaces (APIs), that makes it easy to access such data via Python.
    1.6 Data Cleaning ✔ Step3
        - Data cleaning is one of ways to preprocess data among data Cleaning, Integration, Reduction, and Transformation
        - Key aspects of Machine Learning Workflow depend on cleaned data: Observations, Labels, Algorithms, Features, Model
          (Algorithms: Computer programs that estimate models based on available data, Model: Hypothesized relationship between observations and data)
        - For missing data → Remove, Impute, Mask.  /  For outliers → Remove, Assign, Transform, Predict, Keep.
    1.7 Exploratory Data Analysis (EDA) ✔ Step3
        - Exploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods.
        - Visualization can be created in multiple ways such as Matplotlib, Pandas (via Matplotlib), and Seaborn
    1.8 Feature Engineering and Variable Transformation ✔ Step3
        Variables must often be transformed before they can be included in models. ex. log and polynomial transformations, Encoding (non-numeric → numeric), Scaling
        1.8.1 Feature encoding
            - Binary encoding, One-hot encoding, Ordinal encoding
        1.8.2 Feature scaling
            - Standard scaling, Min-max scaling, Robust scaling
            - ex. Log transforming skew variables
    1.9 Estimation and Inference
        1.9.1 Esitimation:  Estimation is the application of an algorithm, for example taking an average
        1.9.2 Inference:  Inference involves putting an accuracy on the estimate (e.g. standard error of an average, confidence interval)        
    1.10 Hypothesis Testing
        - Hypothesis testing is an act in statistics whereby an analyst tests an assumption regarding a population parameter.
        - Hypothesis testing is used to assess the plausibility of a hypothesis by using sample data.
        - In hypothesis testing, an analyst tests a statistical sample, with the goal of providing evidence on the plausibility of the null hypothesis.

Brief description of the data set and a summary of its attributes
Initial plan for data exploration
Actions taken for data cleaning and feature engineering
Key Findings and Insights, which synthesizes the results of Exploratory Data Analysis in an insightful and actionable manner
Formulating at least 3 hypothesis about this data
Conducting a formal significance test for one of the hypotheses and discuss the results 
Suggestions for next steps in analyzing this data
A paragraph that summarizes the quality of this data set and a request for additional data if needed