1. Introduction to Artificial Intelligence and Machine Learning
    1.1 What is AI, ML, DL ?
        - Artificial Intelligence:  Artificial Intelligence is a branch of computer science dealing with the simulation of intelligent behavior in computers.
        - Machine Learning:  Algorithms whose performance improve as they are exposed to more data over time
        - Deep Learning:  Subset of machine learning in which multilayered neural networks learn from vast amounts of data
    1.2 History of AI
        - AI has experienced cycles of AI winters and AI booms. 
        - AI solutions include speech recognition, computer vision, assisted medical diagnosis, robotics, and others.   
    1.3 Modern AI, Applications
        - Factors that have contributed to the current state of Machine Learning are: bigger data sets, faster computers, open source packages, and a wide range of neural network architectures. 
    1.4 Machine Learning Workflow ✔
        - Step1. Problem Statement → S2. Data collection → S3. Data Exploration & Data preprocessing (70~80%) → S4. Modeling → S5. Valiation → S6. Decision making & Development
    1.5 Machine Learning Vocabulary 
        - Target:  category or value you are trying to predict
        - Label:  the value of the target for a single data point
        - Features:  explanatory variables used for prediction
        - Examples:  an observation or single data point within the data
    1.6 Retrieving Data
        Retrieving data from multiple data sources such as SQL databases, NoSQL databases, APIs, and Cloud data sources
        1.6.1 CSV Files
            - Comma-seperated (CSV) files consists of rows of data, seperated by commas.
        1.6.2 JSON Files
            - JavaScript Object Notation (JSON) files are a standard way to store data across platforms.
        1.6.3 SQL Databases
            - Structured Query Language (SQL) represents a set of relational databases with fixed schemas.
            - ex. Microsoft SQL Server, Postgres, MySQL, AWS Redshift, Oracle DB, Db2 Family
        1.6.4 NoSQL Databases
            - Not-only SQL (NoSQL) databases are not relational, vary more in structure.
            - Most NoSQL databases store data in JSON format.
            - ex. Document databases (mongoDB, couchDB), Key-value stores (Riak, Voldemort, Redis), Graph databases (Neo4j, HyperGraph), Wide-column stores (Cassandra, HBase)
        1.6.5 APIs and Cloud Data Access
            - A variety of data providers make data available via Application Programming Interfaces (APIs), that makes it easy to access such data via Python.
    1.6 Data Cleaning ✔ Step3
        - Data cleaning is one of ways to preprocess data among data Cleaning, Integration, Reduction, and Transformation
        - Key aspects of Machine Learning Workflow depend on cleaned data: Observations, Labels, Algorithms, Features, Model
          (Algorithms: Computer programs that estimate models based on available data, Model: Hypothesized relationship between observations and data)
        - For missing data → Remove, Impute, Mask.  /  For outliers → Remove, Assign, Transform, Predict, Keep.
    1.7 Exploratory Data Analysis (EDA) ✔ Step3
        - Exploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods.
        - Visualization can be created in multiple ways such as Matplotlib, Pandas (via Matplotlib), and Seaborn
    1.8 Feature Engineering and Variable Transformation ✔ Step3
        Variables must often be transformed before they can be included in models. ex. log and polynomial transformations, Encoding (non-numeric → numeric), Scaling
        1.8.1 Feature encoding
            - Binary encoding, One-hot encoding, Ordinal encoding
        1.8.2 Feature scaling
            - Standard scaling, Min-max scaling, Robust scaling
            - ex. Log transforming skew variables
    1.9 Estimation and Inference
        1.9.1 Esitimation:  Estimation is the application of an algorithm, for example taking an average
        1.9.2 Inference:  Inference involves putting an accuracy on the estimate (e.g. standard error of an average, confidence interval)        
    1.10 Hypothesis Testing
        - Hypothesis testing is an act in statistics whereby an analyst tests an assumption regarding a population parameter.
        - Hypothesis testing is used to assess the plausibility of a hypothesis by using sample data.
        - In hypothesis testing, an analyst tests a statistical sample, with the goal of providing evidence on the plausibility of the null hypothesis.
        - significance leve, p-value, ex. https://www.youtube.com/watch?v=KS6KEWaoOOE

2. Supervised Machine Learning: Regression
    2.1 Introduction to Supervised Machine Learning and Linear Regression
        2.1.1 Types of Machine Learning
            - y_p = f(Ω, x)  where x=input, y_p=output(values predicted by the model), Ω=parameters
            - J(y, y_p): Loss, Machine Learning model chooses parameters Ω to minimize loss J by using features x and outcom e y.
            - cf. hyperparameters is a parameter that is not learned directly from the data, but relates to implementation of training our ML model.
            - ex. Regression's output(y) is numeric, and Classification's output(y) is categorical.
        2.1.2 Interpretation and Prediction
            - Interpretation is to train a model to find insights from the data especiall from the Ω.
            - Prediction is to make the best prediction comparing y_p with y.
        2.1.3 Linear Regression 
            - Making our target variable normally distributed often will lead to better results 
              → Check normality by visually and using a statistical data (ex. from scipy.stats.mstats import normaltest)
            - Try some common transformations to try and get y to be normally distributed:  ex. Log, Square root, Box cox
            2.1.3.1 Process of Linear Regression ✔ Step4. Modelling
                - 1) Create X and Y
                - 2) Polynomial Features of X 		→	X_pf = pf.fit_transform(X)  where, pf = PolynomialFeatures(degree=2, include_bias=False)
                - 3) Train test split of X_pf, Y 		→ 	X_train, X_teest, Y_train, Y_test = train_test_split(X_pf, Y, test_size=0.3, random_state=42)
                - 4) Standard Scalar of X_train 		→ 	X_train_s = s.fit_transform(X_train)  where, s = StandardScaler()
                - 5) normalize Y_train 			→ 	Y_train_bc = boxcox(Y_train)[0], lambda1 = boxcox(Y_train)[1]
                - 6) fit model 				→ 	lr(X_train_s, Y_train)
                - 7) Standard Scalar of X_test 		→ 	X_test_s = s.transform(X_test)
                - 8) Predict normalized Y_pred 		→ 	Y_pred_bc = lr.predict(X_test_s)
                - 9) Inverse of Y_pred_bc			→	Y_pred = inv_boxcox(Y_pred_bc, lambda1) 
                - 10) Determine R^2 b/w Y_test and Y_pred	→	r2_score(y_pred, y_test)  where, from sklearn.metrics import r2_score
            2.1.3.2 One-hot-encoding for categorical variables
            2.1.3.3 Scaling variables for linear regression
                - Scaling doesn't have an effect on the plain-linear regression result
            2.1.3.4 Visualize the result (ex. Scatter plot)
    2.2 Data Splits and Cross Validation ✔ Step5. Validation
        2.2.1 Training and Test Splits
            - Splitting your data into a training and a test set can help you choose a model that has better chances at generalizing and is not overfitted.
            - ex. X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.3, random_state=42)
        2.2.2 Cross Validation
            - Find the best model of a complexity where training and cross validation errors are both low
            - Training error tends to decrease with a more complex model. 
            - Cross validation error generally has a u-shape. It decreases with more complex models, up to a point in which it starts to increase again. 
            2.2.2.1 K-fold Cross Validation:  Using each of k subsamples as a test sample
                - ex. kf = KFold(shuffle=True, random_state=72018, n_splits=3)
            2.2.2.2 Leave One Out Cross Validation:  Using each observation as a test sample
            2.2.2.3 Stratified Cross Validation:  K-fold Cross Validation with representative samples
        2.2.3 ex. LASSO (Least Absolute Shrinkage and Selection Operator)
            - ex1. las = Lasso(alpha=best_alpha, max_iter=100000)
                     estimator = Pipeline([("make_higher_features", pf), ("scaler", s), ("lasso_regression", las)])  #where, s = StandardScaler()
                     predictions = cross_val_predict(estimator, X, y, cv=kf)  #where, kf = KFold(shuffle=True, random_state=72018, n_splits=3)
                     score = r2_score(y, predictions)
            - ex2. best_estimator = Pipeline([("make_higher_degree", PolynomialFeatures(degree=2, include_bias=False)), 
				  ("scaler", s), ("lasso_regression", Lasso(alpha=best_alpha, max_iter=100000))])
	       best_estimator.fit(X, y)
	       best_estimator.score(X, y)
        2.2.4 Hyperparameter selection
            - Use GridSearchCV
            - ex. from sklearn.model_selection import GridSearchCV
	     estimator = Pipeline([  ("polynomial_features",  PolynomialFeatures()),  ("scaler", StandardScaler()),  ("ridge_regression", Ridge())  ])
	     params = {  'polynomial_features__degree': [1, 2, 3],  'ridge_regression__alpha': np.geomspace(4, 20, 30)  }
	     grid = GridSearchCV(estimator, params, cv=kf)
	     grid.fit(X, y)
	     y_predict = grid.predict(X)
	     r2_score(y, y_predict)
	     # grid.best_score_, grid.best_params_
	     # grid.best_estimator_.named_steps['ridge_regression'].coef_
    2.4 Regularization Techniques: Ridge, LASSO, and Elastic Net
        2.4.1 Bias and Variance
            - Bias is a tendency to miss. ex. Underfitting (High bias, Low variance), low-order function
            - Variance is tendency to be inconsistent  ex. Overfitting (Low bias, High variance), high-order function
        2.45.2 Regularization and Feature selection
            - M(w) + λR(w) : Adjusted cost function  where M(w): model error, R(w): function of extimated parameters, λ: regularization strength parameter
            - This λ (lambda) adds a penalty proportional to the size of the estimated model parameter, or a function of the parameter.
            - Regularization performs feature selection by shrinking the contribution of features.
            - Feature selection can also be performed by removing features.
        2.4.3 LASSO, Ridge regression and Elastic Net
            - In LASSO regression, the complexity penalty λ (lambda) is proportional to the absolute value of coefficients. (L1-abs, L2-squared)
            - In Ridge regression, the complexity penalty λ (lambda) is applied proportionally to squared coefficient values.
            - Elastic Net, an alternative hybrid approach, introduces a new parameter α(alpha) that determines a weighted average of L1 and L2 penalties.
        2.4.4 Recursive Feature Elimination (REF)
            - RFE applies the model, measures feature importance, and recursively removes less important features to the desired number of features.
    2.5 Polynomial Regression
        - "Linear Regression" means linear combinations of features not linear relationship between features(variables)
        - ex. y(x) = β0 + β1*x + β2*x^2 + β3*x^3   or   y(x) = β0 + β1*x1 + β2*x2 + β3*x1*x2  are also linear regression models.
        - Polynomial terms help you capture nonlinear effects of your features. (Capture the relation of the outcome with features of higher order.)


3. Supervised Machine Learning: Classification
4. Unsupervised Machine Learning
5. Deep Learning and Reinforcement Learning
6. Specialized Models: Time Series and Survival Analysis