Descriptive, Inferential statistics

1. Data and visualization
    1.1 Cases, variables and levels of measurement
        - Levels of measurement: categorical(nominal, ordinal), quantitative(interval, ratio)
    1.2 Data matrix, Frequency table
    1.3 Graphs, Shapes of distributions
        1.3.1 Categorical (nominal, ordinal)
            - Pie chart, Bar graph 
        1.3.2 Quantitative (interval, ratio)
            - Dotplot, Histogram(bell shaped(symmetric), unimodal or bimodal, skewed distribution
    1.4 Measures of central tendency and dispersion
        1.4.1 Graph / Center (mode, median, mean)
        1.4.2 Mode: Value that occurs most frequently (the most common outcome) - categorical
        1.4.3 Median: the middle value of your observations when arranged from the smallest to the largest
        1.4.4 Mean: the sum of all the values divided by the number of obeservations
    1.5 Range, Interquartile range and box plot (measures of variablity, speration of data)
        1.5.1 Range: highest value - lowest value
        1.5.2 Interquartile range: divide range Q1(0-25%), Q2(50%=median), Q3(75-100%), Q4(0-100%) -> IQR(Interqutile range) = Q3 - Q1
    1.6 Variance and standard deviation (measures of variablity, speration of data)
        1.6.1 Variance: ∑(x - mean(x))^2 / (n - 1), larger variance -> larger variablity -> the more the values are spread out around the mean
        1.6.2 Standard deviation: s = Sqrt(Variance) = Sqrt(∑(x - mean(x))^2 / (n - 1)): the average distance of an observation from the mean
        * The explained variance is the percentage of the variance in the dependent variable that can be explained using the formula of the regression line. You can measure this with r-squared. 
    1.7 Z-scores and example
        1.7.1 Z-score: number of standard deviations removed from the mean (Z = (x - mean(x)) / s)
        1.7.2 Z-score makes it esay to see whether a specific score is relatively common or exceptional

2. Correlation and Regression
    2.1 Crosstabs(tables), Scatterplots
        2.1.1 contingency table -> nominal variables, ordinal varibables
        2.1.2 scatterplot -> quantitative variables (show the relations between data like strong or weak correlation)
    2.2 Pearson's r
        2.2.1 Direction(+, -) and strength(-1 to +1) of linear correlation between two variables with one number
        2.2.2 Compute Pearson's r = ∑ (ZxZy) / (n - 1)
        2.2.3 Check scatterplot before you calculate Pearson's r. If it's not a linear relation, the Pearson's r just represent if it's a weak or strong linear correlation
    2.3 Regression analysis
        2.3.1 How to find a regression line? -> Find the smallest sum of squared residuals
        2.3.2 Compute regression line: y^ = a + bx, where b = r(Sy/Sx), a = mean(y) - b*mean(x) (Sy: standard deviation of y, Sx: Standard deviation of x)
					       ( b = ∑(x - mean(x))(y - mean(y)) / ∑(x - mean(x))^2 )
        2.3.3 How well does the regression line fit the data? -> Accuracy of prediction -> r^2 (conjunction of x and y)
            2.3.3.1 Pearson's r = direction and strength of relation
            2.3.3.2 Interpretation1. r^2: How much better a regression line predicts your dependent variable than the mean of that variable
		   ex. r^2 = 0.69 -> prediction error is 69% smaller than when you use the mean 
            2.3.3.3 Interpretation2. r^2: How much of the variance in your dependent variable is explained by your independent variable
		   ex. r^2 = 0.69 -> 69% of the variance in the grades(Y) can be predicted by the previous grades(X)
    2.4 Caveats and examples
        2.4.1 Correlation is no causation

3. Probability
    3.1 Randomness
        • randomness is the apparent or actual lack of pattern or predictability in events
        • A random sequence of events, symbols or steps often has no order and does not follow an intelligible pattern or combination
    3.2 Probability
        • Probability is a way to quantify randomness
        • Sample space, event, tree diagram with quantifying probabilities
    3.3 Basic set-theoretic concepts
        • Disjoint/mutually exclusive: two or more events in a sample space that do not share any outcomes
        • Collectively exhaustive: mutiple events which together fill-up the complete sample space
        • ∑ P(disjoint events) ≤ 1, ∑ P(collectively exhaustive events) = 1
    3.4 Conditional probability & Independence
        • Probability:  The probability of an outcome is interpreted as the long-run proportion of the time that the outcome would occur, if the experiment were repeated indefinitely.
        • That is, probability is long-term relative frequency.
        3.4.1 Joint and marginal probabilities
            • Joint probabilities: Probabilities for the intersection of certain outcomes of the variables
            • Marginal probabilities: Probabilities for an outcome of each individual variable
        3.4.2 Conditional probabilities
            • The probability of an event, given that another event occurs , ex. P(A | B) = P(A ⋀ B) / P(B), P(A ⋀ B) = P(A | B) * P(B)
        3.4.3 Independence between random events
            • Independence: knowing the outcome of one event does not influence your knowledge about the outcome for the others
            • P(A | B) = P(A), P(B | A) = P(B), P(A ⋀ B) = P(A) * P(B) -> Implies indepence
    3.5 Conditional probability, decision trees and Baye's Law
        • P(A | B) * P(B) = P(B | A) * P(A) ↔ P(A | B) = P(B | A) * P(A) / P(B) = P(B | A) * P(A) / ∑(P(B | A_n) * P(A_n))

4. Probability distributions
    4.1 Random variables and probability distributions
        4.1.1 Random variable: Variable whose possible values are numerical outcomes of a random phenomenon
            4.1.1.1 Discrete random variable: countable number of distinct values
            4.1.1.2 Continuous random variable: infinite number of possible values
        4.1.2 Probability distribution: specifies the probabilites for each of the values that a random variable may take
            4.1.2.1 Probability distribution for discrete variable is called 'probability mass function'
            4.1.2.2 Probability distribution for continuous variable is called 'probability density function'
    4.2 Cumulative probability and distributions
        4.2.1 The mean of a random variable
            4.2.1.1 the mean of a discrete random variable
                • probability-weighted average of all possible values that the random variable can take, μ_x = E(x) = ∑[xP(x)] 
            4.2.1.2 the mean of a continuous random variable
                • average of all possible values that the random variable can take, μ_x = E(x) = intelgra[-∞, ∞](xf(x) dx)
            4.2.1.3 Combining different random variables
                • x → a+bx, then μ_x → μ_{a + bx}
                • μ_{x+y} = μ_x + μ_y
        4.2.2 The variance of a random variable
            4.2.2.1 discrete X: var(X)=∑(x_i−μ)^2*P_i(x_i)
            4.2.2.2 continuous X: integral[-∞, ∞]((x-μ)^2*f(x) dx)
    4.3 The normal distribution
        4.3.1 Functional form of the normal distribution
            • A random variable X is normally distributed with mean μ and standard deviation σ, then one may write X ~ N(μ, σ^2) 
            • Normal probability distribution = Gaussian distribution
            • Cumulative normal probability distribution = Sigmoid shape
            • Form of normal distribution(pdf, probability density function), f(x) = 1/(σ*sqrt(2π) * e^{-0.5 *{(x - μ)/σ}^2}
        4.3.2 The normal distribution: probability calculations
            • μ ± σ → 0.68,  μ ± 2σ → 0.95,  μ ± 3σ → 0.997 
        4.3.3 The standard normal distribution (X ~ N(0, 1), which means μ = 0, σ = 1)
            • Normal probability distribution → the standard normal distribution (z-distribution) 
            • Standardize the normal probability distribution to the standard normal distribution (z-distribution), then utilize the z-table
            • z = (x - μ)/σ
            • ex. P(x ≤ 6)? → P((x - μ)/σ = z ≤ (6 - μ)/σ) → Use z-table
    4.4 The binomial distribution
            • binomial distribution = discrete probability distribution
            • x = #success, n = #trials, p = probability of success
            • discrete probability distribution, μ(mean) = np, σ(standard deviation) = sqrt(np(1 - p))
            • Formula for the binomial probability distribution, P(x) = n!/(x!(n - x)!) * p^x * (1 - p)^{n - x}, where x = 0, 1, 2, ... , n
            • Formula for the cumulative probability distribution, F(x) = P(X ≤ x) = ∑[k=0 .. x] n!/(k!(n - k)!) * P^k * (1 - p)^{n - k}

5. Sampling distributions
    5.1 Sample and sampling
        5.1.1 Sample
            • Sample → Descriptive statistics → statiscal data
            • Methods for summraizing sample data are called Descriptive statistics
            • Represented by Roman letters, (ex. mean(x), s for standard deviation)
            • univariate analysis → mode, mean, meadian
            • bivariate analysis → Pearon's R, regression analysis
        5.1.2 Population
            • Sample → Descriptive statistics → statiscal data → Inferential statistics → Population → statiscal data (parameters)
            • Represented by Greek letters, (ex. μ = mean, σ = standard deviation)
        5.1.3 Sampling
            • Simple random sample: each subject in the populatin has a same change of being selected 
            • simple random sample Bias (undercoverage, sampling bias, nonresponse bias, response bias)
    5.2 Sampling distribution of sample mean and Central limit theorem
        5.2.1 The sampling distribution of the sample mean (Sampling distribution)
            • infinite number of samples → the mean of that distribution = mean in the population (Sampling distribution)
            • Sampling distribution ≠ Sample distribution ≠ Data distribution
        5.2.2 Central limit theorem
            • No matter how a variable is distributed in the population, the sampling distribution of the sample mean is approximately normal as long as the sample size is at least thirty
            • Mean of sampling distributiom μ_{mean(x)} = population mean μ,  σ_{mean(x)} = σ / sqrt(n) where n is sample size
        5.2.3 Three distributions
    5.3 Three distributions
        5.3.1 Population distribution (Greek letters, μ = mean, σ = standard deviation)
        5.3.2 Data/sample distribution (Roman letters, mean(x), s)
        5.3.3 Sampling distribution (Greek letters with mean(x), μ_{mean(x)} = μ, σ_{mean(x)}  = σ / sqrt(n))
    5.4 Sampling distribution of sample proportion
        - Sampling distribution of sample mean → approximately bell-shaped if population is normally distributed or if sample size is sufficiently large (≥30)
        - Sampling distribution of the sample proportion → approximately bell-shaped if nπ ≥ 15 and n(1 - π) ≥ 15, where n is sample size, and π is population proportion
        - Standard deviation of sampling distribution of sample proportion → σ_p = sqrt(π(1 - π)/n), where n is sample size, π is population proportion

6. Confidence Intervals (Z_90% = ±1.645, Z_95% = ±1.96, Z_99% = 2.58) 
    6.1 Inference and confidence interval for mean (T distribution with df (degrees of freedom, n - 1))
        6.1.1 Statiscal inference (Estimation / Hypothesis testing)
            - Sample information → Infer total population information
        6.1.2 Estimation 
            6.1.2.1 Point estimate 
                - ex. Sample mean(x) = 2.6 → Estimate population mean, μ = 2.6? (not clear how close)
            6.1.2.2 Interval estimate
                - Sample mean(x) → Estimate population mean interval, μ = 2.6 ~ 2.9 with confidence level 0.95 (95% confidence interval)
        6.1.3 Confidence Interval for mean with known population standard deviation(σ)
            - Confidence Interval → estimate a population mean → in 95% of the samples the population value will fall within the confidence interval
            - mean(x) ± 1.96σ_{mean(x)}  or  mean(x) ± Z_{95%}σ_{mean(x)}, where σ_{mean(x)} = σ / sqrt(n), but we usually don't know σ which is sd of population
        6.1.4 Confidence Interval for mean with unknown population standard deviation(σ)
            - Use sample sd and T-distribution to add error 
            - mean(x) ± t_{95%}(se), where se = s / sqrt(n), where se = the estimated standard deviation of the sampling distribution of the sample mean
        6.1.5 assumptions
            - sample should be random sample
            - population approximately normal population distribution (but T-distribution method robust against the violation)
            - be wary of extreme outliers
    6.2 Confidence interval for proportion (Z distribution) and confidence levels
        6.2.1 CI for proportion
            - Confidence Interval for a proportion → sampling distribution of the sample proportion
            - : μ_p = π, σ_p = sqrt(π(1 - π)/n), where n is sample size, and π is population proportion
            - Confidence Interval for a proportion → p ± 1.96σ_p = p ± Z_{95%}σ_p , σ_p = sqrt(π(1 - π)/n)
            - We don't know the population proportion π, so we can write CI for a proportion as p ± Z_{95%}(se), where se = sqrt(p(1 - p)/n), p = sample proportion
            - where se = standard error (the estimated standard deviation of the sampling distribution the standard error)
            - Assumtion: at least 15 successes and 15 failures which are np ≥ 15 and n(1 - p) ≥ 15
        6.2.2 Confidence levels
            - Constructing a confidence interval
            - 1. Which confidence level?
            - 2. Proportion or mean?
                  - Proportion → Z distribution
                  - Mean → T distribution → Compute df (degrees of freedom, n - 1)
            - 3. Compute endpoints of interval
            - 4. Interpret results substantively  
    6.3 Sample size and example
        6.3.1 Choosing the sample size
            - 1. magnitude of desired margin of error (smaller margin of error → larger sample size)
            - 2. confidence level (larger confidence level → larger sample size)
            - 3. variability (larger the standard deviation → larger your sample size)
        6.3.2 Sample size for means
            - n = (σ^2 * z^2) / m^2 (ex. confidence level: 95% -> z = 1.96, σ = educated guess, m = margin of error)
        6.3.3 Sample size for proportions
            - n = p(1 - p)z^2 / m^2 (ex. confidence level: 99% -> z = 2.58, p(1 - p) largest possible value happens p = 0.5)

7. Hypotheses and significance tests
    7.1 Hypotheses and significance tests
        7.1.1 Hypotheses
            7.1.1.1 Null hypothesis (H_0)
                - the parameter you're interested in takes a specific value
                - assumed to be true unless convincing evidence proofs (ex. In courtroom, Innocent)
                - It will be rejected (false) if the data in your sample suggest that it is a highly unlikly expectation
            7.1.1.2 Alternative hypothesis (H_a)
                - claims that the parameter you're interested in falls within an alternative Range or values
                - accepted if convincing evidence shows this is the case (ex. In courtroom, Guilty)
        7.1.2 Test about proportion
            - significance test about population proportion
            - test statistic = z = (p - π_0) / se_0, where se_0 = sqrt(π_0(1 - π_0) / n) 
             , [p: sample proportion, π_0(H_0): null hypothesis population proportion, se_0: standard error assumed null hypothesis]
            - if z-score's probability is less than the significance level (α), e.g., 0.05, then we reject the null hypothesis π_0(H_0) -> so H_a
            - two-tailed test is common
        7.1.3 Test about mean
            - significance test about population mean
            - test statistic = t = (mean(x) - μ_0) / se, where se = s / sqrt(n) 
             , [mean(x): sample mean, μ_0: null hypothesis population mean, s: sample standard deviation]
             , we use t-score (t-distribution) rather than z-score because usually we don't know σ: population standard deviation
    7.2 Step-by-step plan and confidence interval
        7.2.1 Step-by-step plan
            - 1. Proportion or mean?
            - 2. Formulate hypothesis
            - 3. Check assumptions
            - 4. Determine α (significance level)
            - 5. Compute test statistic
            - 6. Draw sampling distribution
            - 7. Find location of test statistic
            - 8. Reject H_0?
            - 9. Interpre findings
        7.2.2 Significance test and confidence interval
            - sample info → population info : inferential statistics
            - 1. estimation (confidence intervals) / 2. significance tests
    7.3 Type I and Tpye II errors and example (Z_90% = ±1.645, Z_95% = ±1.96, Z_99% = 2.58) 
        - Type I error, if the null hypothesis is true and you decide to reject
        - Type II error, if the null hypothesis is false and you decide not to reject
        - Power, The power of a test is the probability of rejecting the null hypothesis given that it is false

